{"cells":[{"cell_type":"markdown","source":["\n","\n","<center><a href=\"https://githubtocolab.com/hirwa-nshuti/UnderstandNN/blob/main/Convnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" width=\"150\" height=\"50\"/></a></center>\n","\n"],"metadata":{"id":"Q7OkA1aLmOoF"}},{"cell_type":"markdown","source":["#**Convolutional Neural Network**\n","\n","*The notebook for deep introduction to Convolutional Neural Nets, implementations from scracth using Numpy and other advanced notes on written papers with summary of what I'll be learning.*"],"metadata":{"id":"4S3-lhysyl_9"}},{"cell_type":"markdown","source":["## Contents\n","\n","- [1 - Introduction](#1)\n","  - [1.1 - Convolution Layer](#1-1)\n","  - [1.2 - Pooling Layers](#1-2)\n","- [2 - Convolutional layer with Numpy](#2)\n","  - [2.1 - Zero Padding](#2-1)\n","  - [2.2 - Convolve forward](#2-1)\n"],"metadata":{"id":"Tt6nFyvIzD-8"}},{"cell_type":"markdown","source":["<a name='1'></a>\n","## 1 - Introduction\n","\n","Short notes on Convolutional Neural Nets."],"metadata":{"id":"8x1fGcD5y2BY"}},{"cell_type":"markdown","metadata":{"id":"3eeHZ_2Pykf6"},"source":["A convolutional Neural network(CNN or ConvNet) is a Deep Learning Neural Network architecture which learns directly from data, eliminating the need for manual feature extraction. CNNs are particularly most useful for finding patterns in images to recognize objects, faces, and scenes. And they can also be quite effective when used for classifying non-image data such as audio, time series etc.\n","\n","Refering to perceptrons the CNNs are regularized multilayer perceptrons. The regularization is to prevent them from proning to overrfitting on the training Data. And we can say that it is a feedforward neural network with input layer, hidden layer and output layer. And in the hidden layer the inputs are masked by activation function and later the **[ Convolution](https://en.wikipedia.org/wiki/Convolution)** operation.\n","\n","*The three main layers of a CNN are Convolutional Layer, Pooling Layer, and Fully-Connected Layer. A simple example for CNN Architecture is [INPUT - CONV - RELU - POOL - FC]*\n","\n","It is typically a stack of input layer, followed by convolutional laye, followed by activation, followed by pooling layer and lately feed the results to a fully connected layer network.\n"]},{"cell_type":"markdown","source":["<a name='1-1'></a>\n","### 1.1 - Convolution layer \n","\n","**The Convolution layer** Takes the input images and pass them to convolution filters and each filter activates certain features of image fed to the network. This can be refered to as edge detection. When using python the convolution is implemented as `np.convolve`  in Tensorflow it is `tf.nn.conv2d` and in keras it is `tf.keras.layers.Conv2D` while when using PyTorch it can be implemented as `torch.nn.Conv2d`\n","\n","#### ***Types of convolutions*** \n","In convolution types that I will cover here are **valid**, **same** and **strided** convolution.\n","\n","#### **valid convolution** \n","Valid convolution we input an (n, n) dimmensional array and pass it to (f, f) diminsional convolutional filter to get (n-f+1, n-f+1) dimensional array as output.\n","As this may result into getting shrinked output array as we stack many convolution layers it is a better practice to introduce padding, strides filter to achieve the desired output array size.\n","\n","#### **Same convolution** \n","Same convolution the output array of the convolution step is always of the same dimension as the input array. Here padding helps to achieve this task and the padding values is obtained as  $ padding = \\frac{f-1}{2}$ suppose we have an (n, n) dimensional array and we apply the padding p.\n","After padding an array the dimensions become (n+2p, n+2p) dimensional array and if we pass this into (f, f) filter the output array is (n+2p-f+1, n+2p-f+1) and according to the padding value the $2p = f-1$ means after the convolution the output array will have (n, n) dimension.\n","\n","#### **Strided Convolution**\n","In strided convolution we add the stride property to govern steps being taken by filter across the image matrix. If we pass the (n, n) image to a filter (f, f) with padding p, and stride s, the output of a strided convolution is ($\\frac{n+2f-p}{s}+1$,  $\\frac{n+2f-p}{s}+1$).\n"],"metadata":{"id":"bsMepVfdl8Rm"}},{"cell_type":"markdown","source":["<a name='1-2'></a>\n","### 1.2 - Pooling layer \n","\n","The pooling layer main role is to perform a non linear operation of reducing the number of parameters to that the network needs to learn efficiently. Hence controling the overfitting of the network. The most common used pooling filter is 2x2\n","\n","There are two types of pooling the max and average pooling\n","\n","#### **Average Pooling**\n","The average pooling takes the average of the values of the input window size(determined by pooling size mostly 2x2) and then the window is strided according to the stride value provided.\n","\n","#### **Max Pooling**\n","Quite similar to average pooling except that the pooling is done by taking the maximum value over the input window size and then the window is strided according to the stride value provided."],"metadata":{"id":"29TmZxFQH8PM"}},{"cell_type":"markdown","source":["<a name=\"#2\"></a>\n","## 2 - Convolutional layer with Numpy"],"metadata":{"id":"8VvBVw9M5VZN"}},{"cell_type":"markdown","source":["Importing the libraries to use"],"metadata":{"id":"JR44zVWp6TQ5"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0)\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","np.random.seed(5)"],"metadata":{"id":"VrRWKdkK5klP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"2-1\"></a>\n","\n","### 2.1 - Zero Padding\n","\n","Zero padding helps to control the shrinkage of dimension of input array after applying filters larger than 1x1, and to avoid loosing information at the boundaries, forexample: when weights in a filter drop rapidly away from its center."],"metadata":{"id":"ZUjEvqfM6vmp"}},{"cell_type":"code","source":["def zero_pad(x, pad=1):\n","    \"\"\"\n","    The function to padd with zeros all the images from dataset x.\n","\n","    Parameters\n","    ----------\n","    x\n","        a python numpy array of shape (examples, height, width, channels) \n","        simply a batch of examples images.\n","    pad\n","        an integer showing amount of padding to be masked on the images inputs\n","        default is 1.\n","\n","    Returns\n","    -------\n","    ret\n","        a python numpy array with elements padded with zeros,\n","        and it must have the same shape as x.\n","    \"\"\"\n","    return np.pad(x, ((0,0), (pad, pad),(pad, pad), (0,0)), mode='constant', constant_values=(0,0))"],"metadata":{"id":"qic1mjFS6777"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Some padded examples\n","x = np.random.randn(5, 4, 4, 1)\n","padded_x = zero_pad(x, 1)\n","print(f\"x.shape={x.shape}\")\n","print(f\"padded_x.shape={padded_x.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xra7iIgy99TZ","executionInfo":{"status":"ok","timestamp":1658293206686,"user_tz":-330,"elapsed":406,"user":{"displayName":"Felix Hirwa Nshuti","userId":"14275747974412463959"}},"outputId":"027f03b9-977b-48af-a759-7d89aa0c4256"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x.shape=(5, 4, 4, 1)\n","padded_x.shape=(5, 6, 6, 1)\n"]}]},{"cell_type":"markdown","source":["**Plotting original and padded images**"],"metadata":{"id":"MP029R6t-spU"}},{"cell_type":"code","source":["fig, ax = plt.subplots(1, 2)\n","ax[0].set_title(\"x\")\n","ax[0].imshow(x[1, :, :, 0])\n","\n","ax[1].set_title(\"padded_x\")\n","ax[1].imshow(padded_x[1, :, :, 0])\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"id":"YbdVkhQV-RHr","executionInfo":{"status":"ok","timestamp":1658293577988,"user_tz":-330,"elapsed":549,"user":{"displayName":"Felix Hirwa Nshuti","userId":"14275747974412463959"}},"outputId":"067a3526-1391-4a4c-b7ba-c007a2670a40"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWoAAADHCAYAAAAwLRlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPKElEQVR4nO3df+xddX3H8efLDlKUflcHnQHKrxBnQkgGyjARwxyZW0WmJvuHH7Jl05AssrTBxImSTTaM+2dG48wckx8apYwMmzjmr26WORJBC6KRFhdEWMtIimDzbReUFN77457Ctf32+73f9nvv+fR7n4/khnu/555zXvfL+b5yes6555OqQpLUrlf0HUCSND+LWpIaZ1FLUuMsaklqnEUtSY2zqCWpcRa1pEVJ8pEkX5hn+uNJfvcwl33Y8y5nFrUkNc6ilqTGWdQ9SHJWkmeTvL57fXKSp5O8pedoWsa6wwrXJdmW5GdJbk2yMsmrk9zdbYM/656vHZrvzCT/mWRPks3AiQcs96okTyR5JsmHD5j2iiQfTPLjbvqdSX5tlHnn+RxfSfJ3Q6/vSHLLYf9ijgIWdQ+q6sfAXwBfSPJK4Fbgc1V1T6/BNA2uBH4fOAv4DeB6Bj1wK3A6cBrwHPD3Q/PcDjzAoKD/Bvjj/ROSnA38A3AVcDJwArB2aN4/B94F/HY3/WfAp0ec91D+FLgqycVJrgQuANaP+PmPSvFeH/1J8mXgTKCA36qqX/QcSctYkseBv62qz3SvLwE+VVVnHfC+c4EtVfXqJKcBjwG/WlX/102/HXixqt6d5C+Bs6vqsm7aqxiU8SVV9e9JtgPXVNV/dNNPAv4HOA740HzzLvBZ/hD4RLecd1XVvUf6+2mZe9T9+ifgHAZ/LJa0JmHH0PMngJOTvDLJP3aHIGaBbwGrk6yg2wveX9JD8+138vAyu/c9MzT9dGBTkt1JdgPbgReA14ww73z+FVgB/Gi5lzRY1L1JcjyDPYKbgY8MH7eTxujUoeenAf8LvB94HfDGqpoBLuqmB3gKeHW3tzs8335PDS+zO5R3wtD0HcDbqmr10GNlVT05wrzz+SiD0j8pyeUjznPUsqj780lga1W9F/g34DM959F0eF+Std2OwYeBfwZWMTguvbv7+V/tf3NVPQFsBW5IcmySNwN/MLS8fwEuTfLmJMcCf80v98pngI8mOR0gyZok7xxx3jkluQj4E+CPGBwv/1SSUxb9mziKWNQ96DbUdcCfdT+6Fnh9d2JEGqfbgW8wOO78Y+BGXj7W+1PgPuBrB8xzBfBG4FkGJf75/ROq6mHgfd1yn2JwjHnn0LyfBL4MfCPJnm75bxxx3oMkmenWf01VPVlV/8XgX6W3Jskifg9HFU8mSlOiO5n43oVO1Kk97lFLUuMsaknNSfLVJHvneHyo72x98NCHJDXOPWpJapxFLUmN+5VxLHTVqlW1Zs2acSz6iPzkJz/pO8Kc3vCGN/Qd4ZB27Nix8JsmbHZ2lueee27il2LNzMw0uV1reXj66aeZnZ2dc7seS1GvWbOGG2+8cRyLPiJXXtnmZcpbt27tO8Ihbdiwoe8IB9m4cWMv612zZg0f+9jHelm3lr/rrrvukNM89CFJjbOoJalxFrUkNc6ilqTGWdSS1DiLWpIaZ1FLUuMsak21JOuS/CjJo0k+2HceaS4WtaZWNybgp4G3AWcDl3cjY0tNsag1zS4AHq2qx6rqeeAO4J0LzCNNnEWtaXYKvzwq987uZy9JcnWSrUm2zs7OTjSctJ9FLc2jqm6qqvOr6vyZmZm+42hKWdSaZk8Cpw69Xtv9TGrKSEXtmXEtU98FXpvkzCTHApcxGDFbasqCRe2ZcS1XVbUPuAb4OrAduLOqHu43lXSwUe5H/dKZcYAk+8+MbxtnMGkSquorwFf6ziHNZ5RDHwueGZckjc+SnUwcvoxpz549S7VYSZp6oxT1SGfGhy9jWrVq1VLlk6SpN0pRe2Zcknq04MnEqtqXZP+Z8RXALZ4Zl6TJGWkUcs+MS1J/RipqSZOxb9++iaznyiuvHPs6qmrs6wDYsGHD2Nfxpje9aezrmI9fIZekxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmNs6glqXEWtSQ1zqKWpMZZ1JpaSW5JsivJD/vOIs3HotY0uw1Y13cIaSEWtaZWVX0LeLbvHNJCxnJTppmZGdata29H5frrr+87wpxuu+22viMc0tlntzeO8cqVK/uOIE2Ue9TSPIaHmJudne07jqaURS3NY3iIuZmZmb7jaEpZ1JLUOItaUyvJRuDbwOuS7Ezynr4zSXNxhBdNraq6vO8M0ijco5akxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmNs6glqXFeRy01ZFI3M5vEDcomdbOxFm8cttTco5akxlnUktS4BYva4YokqV+j7FHfhsMVSVJvFixqhyuSpH55jFqSGrdkRT08ZNEzzzyzVIuVpKm3ZEU9PGTRCSecsFSLlaSp56EPSWrcKJfnOVyRlqUkpybZkmRbkoeTrO87kzSXBb9C7nBFWsb2Ae+vqgeTrAIeSLK5qrb1HUwa5qEPTa2qeqqqHuye7wG2A6f0m0o6mEUtAUnOAM4D7j/g5y9dzTQ7O9tHNMmilpIcD9wFbKiqX2rj4auZZmZm+gmoqWdRa6olOYZBSX+xqr7Udx5pLha1plaSADcD26vq433nkQ7FotY0uxC4Crg4yUPd45K+Q0kHcoQXTa2quhdI3zmkhbhHLUmNs6glqXEWtSQ1zqKWpMZZ1JLUOItakho3lsvzdu7cybXXXjuORR+Riy66qO8IczrnnHP6jnBI99xzT98RDvLiiy/2HWFsJvV3M4m/hUlt15PYRlevXj32dczHPWpJapxFLUmNs6glqXEWtSQ1zqKWpMZZ1JLUOItakhpnUUtS4yxqSWqcRa2plWRlku8k+X6Sh5Pc0HcmaS6O8KJp9gvg4qra2w1ye2+Sr1bVfX0Hk4ZZ1JpaVVXA3u7lMd2j+kskzc1DH5pqSVYkeQjYBWyuqvsPmH51kq1Jts7OzvYTUlPPotZUq6oXqupcYC1wQZJzDph+U1WdX1Xnz8zM9BNSU8+iloCq2g1sAdb1nUU60IJFneTUJFuSbOvOjK+fRDBp3JKsSbK6e34c8FbgkX5TSQcb5WTiPuD9VfVgklXAA0k2V9W2MWeTxu0k4HNJVjDYabmzqu7uOZN0kAWLuqqeAp7qnu9Jsh04BbCodVSrqh8A5/WdQ1rIoo5RJzmDwYZ9//zvlCQtlZGLOsnxwF3Ahqo66Dql4cuYfv7zny9lRkmaaiMVdfetrbuAL1bVl+Z6z/BlTCtXrlzKjJI01Ua56iPAzcD2qvr4+CNJkoaNskd9IXAVcHGSh7rHJWPOJUnqjHLVx71AJpBFkjQHv5koSY3z7nlSQ3bv3j2R9ezYsWPs6zjttNPGvg6A9evH/2XpTZs2jX0d83GPWpIaZ1FLUuMsaklqnEUtSY2zqCWpcRa1JDXOopakxlnUktQ4i1pTrRuF/HtJHNlFzbKoNe3WA9v7DiHNx6LW1EqyFng78Nm+s0jzsag1zT4BfAB4se8g0nzGclOmffv2TezmMosxiRvRHI5J3bzmcEzihjeLtXHjxiNeRpJLgV1V9UCSt8zzvquBqwFOPPHEI16vdDjco9a0uhB4R5LHgTsYDIzxhQPfNDzE3MzMzKQzSoBFrSlVVddV1dqqOgO4DPhmVb2751jSnCxqSWqcAwdo6lXVPcA9PceQDsk9aklqnEUtSY2zqCWpcRa1JDXOopakxlnUktQ4i1qSGud11FJDrrjiir4jLJlnn312IuvZtGnTRNbTJ/eoJalxFrUkNW7Bok6yMsl3knw/ycNJbphEMEnSwCjHqH8BXFxVe5McA9yb5KtVdd+Ys0mSGKGoq6qAvd3LY7pHjTOUJOllIx2j7kZqfgjYBWyuqvvHG0uStN9IRV1VL1TVucBa4IIk5xz4niRXJ9maZOvzzz+/1DklaWot6qqPqtoNbAHWzTHtpSGLjj322KXKJ0lTb5SrPtYkWd09Pw54K/DIuINJkgZGuerjJOBzSVYwKPY7q+ru8caSJqMb3HYP8AKwr6rO7zeRdLBRrvr4AXDeBLJIffmdqvpp3yGkQ/GbiZLUOIta066AbyR5IMnVB04cvpppdna2h3iSd8+T3lxVTyb5dWBzkkeq6lv7J1bVTcBNAGeddZZf9FIv3KPWVKuqJ7v/7gI2ARf0m0g6mEWtqZXkVUlW7X8O/B7ww35TSQfz0Iem2WuATUlg8Ldwe1V9rd9I0sEsak2tqnoM+M2+c0gL8dCHJDXOopakxlnUktQ4i1qSGmdRS1LjLGpJapxFLUmNy2Ds2iVeaPI08MQSLe5EoMVbUJprcZYy1+lVtWaJljWyw9yuW/3/cTiWy2dp9XMccrseS1EvpSRbW7yZu7kWp9Vc47acPvdy+SxH4+fw0IckNc6ilqTGHQ1FfVPfAQ7BXIvTaq5xW06fe7l8lqPuczR/jFqSpt3RsEctSVOt2aJOsi7Jj5I8muSDfefZL8ktSXYlaeYG80lOTbIlybYkDydZ33cmgCQrk3wnyfe7XDf0nWlSWt1+F6vVbetIJFmR5HtJ7u47y6iaPPSRZAXw38BbgZ3Ad4HLq2pbr8GAJBcBe4HPV9U5fecBSHIScFJVPdiNWPIA8K6+f18Z3JH/VVW1N8kxwL3A+qq6r89c49by9rtYrW5bRyLJtcD5wExVXdp3nlG0ukd9AfBoVT1WVc8DdwDv7DkTAN3Ap8/2nWNYVT1VVQ92z/cA24FT+k0FNbC3e3lM92hvz2DpNbv9Llar29bhSrIWeDvw2b6zLEarRX0KsGPo9U6O4o1jkpKcAZwH3N9vkoHun5kPAbuAzVXVRK4xW5bbb2vb1mH6BPAB4MW+gyxGq0Wtw5DkeOAuYENVzfadB6CqXqiqc4G1wAVJmjhcpMVpcdtarCSXAruq6oG+syxWq0X9JHDq0Ou13c90CN0x4LuAL1bVl/rOc6Cq2g1sAdb1nWUCltX22/q2tQgXAu9I8jiDw1EXJ/lCv5FG02pRfxd4bZIzkxwLXAZ8uedMzepO2t0MbK+qj/edZ78ka5Ks7p4fx+Dk2iP9ppqIZbP9trptHY6quq6q1lbVGQz+n3yzqt7dc6yRNFnUVbUPuAb4OoOTF3dW1cP9phpIshH4NvC6JDuTvKfvTAz2FK5isIfwUPe4pO9QwEnAliQ/YFBem6vqqLkk6nC1vP0ehla3ranS5OV5kqSXNblHLUl6mUUtSY2zqCWpcRa1JDXOopakxlnUktQ4i1qSGmdRS1Lj/h9Gd5xT7eCN9gAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["<a name=\"2-2\"></a>\n","\n","### 2.2 - Convolve forward\n","\n","In the CNN layer we have two main functions the forward pass and backward pass, during the forward pass we pass the input n_h x n_w dimensional numpy array and perform filtering and max pooling on the input array. The output of a convolved network is called a feature map."],"metadata":{"id":"5ZMcm_tNgDmr"}},{"cell_type":"code","source":["def convolve_forward(prev_out, W, b, h_parameters):\n","    \"\"\"\n","    The forward convolution pass. It receives the input array, perform the\n","    convolution and gives the feature maps.\n","\n","    Parameters\n","    ----------\n","    prev_out\n","        output activations of the previous layer, \n","        numpy array of shape (examples, n_h_prev, n_w_prev, n_channel_prev)\n","    W\n","        Weights numpy array (f, f, n_channel_prev, n_channel)\n","    b\n","        Biases, numpy array of shape (1, 1, 1, n_channel)\n","    hparameters\n","        python dictionary containing \"stride\" and \"pad\n","    \"\"\"\n","    pass"],"metadata":{"id":"S7bG5yCzgHS_"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"name":"Convnets.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}